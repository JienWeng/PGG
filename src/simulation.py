import os
import numpy as np # Ensure numpy is imported
import pandas as pd
from typing import Dict, List, Tuple, Union, Any
import random
from pathlib import Path
import math # Added for factorial

# Import environment and agents
from pgg_environment import PublicGoodsGame
from q_agent import QAgent
from double_q_agent import DoubleQAgent

def calculate_shapley_value(contributions: List[float], r: float) -> List[float]:
    """
    Calculate Shapley values for each player in the public goods game.
    
    Args:
        contributions: List of contributions from each player
        r: Multiplier for public goods
        
    Returns:
        List of Shapley values for each player
    """
    n = len(contributions)
    shapley_values = [0.0] * n
    
    memo_v = {} # Memoization for v(coalition_tuple)

    def v(coalition_indices: List[int]) -> float:
        """Calculate value of a coalition"""
        coalition_tuple = tuple(sorted(coalition_indices)) # Use a hashable type for memoization
        if not coalition_tuple:
            return 0.0
        if coalition_tuple in memo_v:
            return memo_v[coalition_tuple]
        
        coalition_sum = sum(contributions[i] for i in coalition_tuple)
        # Characteristic function: value generated by the coalition, shared among its members
        if not coalition_tuple: # Should be caught by the first check, but defensive
            result = 0.0
        else:
            result = (r * coalition_sum) / len(coalition_tuple) 
        memo_v[coalition_tuple] = result
        return result
    
    players = list(range(n))
    for i in range(n): # For each player i
        other_players = [p for p in players if p != i]
        for k in range(1 << (n - 1)): # Iterate through all subsets of other_players
            coalition_S = [] # Coalition S (does not include player i)
            for j in range(n - 1):
                if (k >> j) & 1:
                    coalition_S.append(other_players[j])
            
            s = len(coalition_S)
            # Corrected Shapley weight: s! * (n - 1 - s)! / n!
            if n == 0: 
                weight = 0.0
            else:
                try:
                    weight = (math.factorial(s) * math.factorial(n - 1 - s)) / math.factorial(n)
                except ValueError: 
                    weight = 0.0 

            # Value of coalition S with player i minus value of coalition S without player i
            val_S_union_i = v(coalition_S + [i])
            val_S = v(coalition_S)
            marginal_contribution = val_S_union_i - val_S
            shapley_values[i] += weight * marginal_contribution
            
    return shapley_values

def calculate_metrics(episode_contributions: List[float], episode_rewards_from_env: List[float], r: float, endowments: List[float]) -> Dict[str, float]:
    """
    Calculate metrics for the current episode.
    Payoffs for logging are calculated here based on the PGG formula.
    """
    n_agents = len(episode_contributions)
    if n_agents == 0: 
        # Return default values if there are no agents or contributions
        metrics = {
            'avg_contribution': 0.0,
            'total_contribution': 0.0,
            'social_welfare': 0.0, # Based on calculated payoffs
            'fairness': 1.0, 
            'action_diversity': 0.0
        }
        # Add default agent-specific metrics if necessary, though n_agents=0 implies no agents
        for i in range(len(endowments)): # Assuming endowments list might exist even if contributions don't
            metrics[f'agent_{i}_contrib'] = 0.0
            metrics[f'agent_{i}_norm_contrib'] = 0.0
            metrics[f'agent_{i}_shapley'] = 0.0
            metrics[f'agent_{i}_payoff'] = 0.0 # Default payoff
        return metrics

    # Basic metrics from contributions
    avg_contribution = np.mean(episode_contributions) if episode_contributions else 0.0
    total_contribution = np.sum(episode_contributions)
    
    # Calculate PGG payoffs explicitly for each agent for logging
    calculated_agent_payoffs = [0.0] * n_agents
    public_good_total = r * total_contribution
    public_good_share_per_agent = public_good_total / n_agents if n_agents > 0 else 0.0
    
    for i in range(n_agents):
        # Payoff = (Endowment - Contribution) + Share of Public Good
        calculated_agent_payoffs[i] = (endowments[i] - episode_contributions[i]) + public_good_share_per_agent
        
    # Social welfare is the sum of these calculated individual payoffs
    social_welfare = np.sum(calculated_agent_payoffs)
    
    # Calculate Shapley values based on actual contributions
    shapley_values = calculate_shapley_value(episode_contributions, r) # Assumes this function is defined elsewhere
    
    norm_contributions = []
    if len(endowments) == n_agents: # Ensure endowments list matches number of contributions
        norm_contributions = [(c / endowments[i] * 100) if endowments[i] > 0 else 0 for i, c in enumerate(episode_contributions)]
    else: # Fallback if endowments list doesn't match, though this indicates an issue
        print(f"Warning: Mismatch between number of contributions ({n_agents}) and endowments ({len(endowments)}) in calculate_metrics.")
        norm_contributions = [0.0] * n_agents # Or handle error appropriately

    max_sv = max(shapley_values) if shapley_values else 0.0
    min_sv = min(shapley_values) if shapley_values else 0.0
    
    if not shapley_values:
        fairness_metric = 1.0 
    elif max_sv == min_sv:
        fairness_metric = 1.0
    elif abs(max_sv) < 1e-9: # Avoid division by zero if max_sv is very small
        fairness_metric = 0.0 if abs(min_sv) > 1e-9 else 1.0 # If both are zero, it's fair
    else:
        fairness_metric = 1.0 - (max_sv - min_sv) / (abs(max_sv) + 1e-9)

    metrics = {
        'avg_contribution': avg_contribution,
        'total_contribution': total_contribution,
        'social_welfare': social_welfare, # Using sum of explicitly calculated payoffs
        'fairness': fairness_metric,
        'action_diversity': len(set(norm_contributions)) / n_agents if n_agents > 0 else 0.0,
    }
    
    # Add individual agent metrics
    for i in range(n_agents):
        metrics[f'agent_{i}_contrib'] = episode_contributions[i]
        metrics[f'agent_{i}_norm_contrib'] = norm_contributions[i] if i < len(norm_contributions) else 0.0
        metrics[f'agent_{i}_shapley'] = shapley_values[i] if i < len(shapley_values) else 0.0
        metrics[f'agent_{i}_payoff'] = calculated_agent_payoffs[i] # Using explicitly calculated payoff

    # If there are more endowments than agents with contributions (e.g. some agents didn't contribute)
    # ensure all potential agents have default entries if not covered.
    # This part might be redundant if n_agents is always derived from episode_contributions and matches endowments.
    if len(endowments) > n_agents:
        for i in range(n_agents, len(endowments)):
            if f'agent_{i}_contrib' not in metrics: # Ensure we don't overwrite
                 metrics[f'agent_{i}_contrib'] = 0.0
                 metrics[f'agent_{i}_norm_contrib'] = 0.0
                 metrics[f'agent_{i}_shapley'] = 0.0
                 metrics[f'agent_{i}_payoff'] = endowments[i] # Default payoff if no contribution = endowment + 0 share

    return metrics

# The run_simulation function needs to pass episode_rewards from env to calculate_metrics
# as the second argument, now named episode_rewards_from_env for clarity in calculate_metrics.

def run_simulation(
    algorithm: str,
    num_episodes: int,
    env: PublicGoodsGame,
    agents: List[Union[QAgent, DoubleQAgent]],
    output_dir: str
) -> Dict[str, List[float]]:
    """
    Run MARL simulation, return and save metrics. Each episode is a single interaction.
    """
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    num_agents = env.num_agents # Use this for initializing metrics_history

    metrics_history = {
        "avg_contribution": [],
        "total_contribution": [],
        "social_welfare": [],
        "fairness": [],
        "action_diversity": []
    }
    # Initialize based on the number of agents the environment is configured for
    for i in range(num_agents):
        metrics_history[f"agent_{i}_contrib"] = []
        metrics_history[f"agent_{i}_norm_contrib"] = []
        metrics_history[f"agent_{i}_shapley"] = []
        metrics_history[f"agent_{i}_payoff"] = []
    
    states = env.reset()
    
    for episode in range(num_episodes):
        # If not the first episode, reset for the new episode
        if episode > 0:
            states = env.reset() 
        
        actions = []
        for agent_idx in range(num_agents):
            action = agents[agent_idx].choose_action(states[agent_idx])
            actions.append(action)
        
        next_states, rewards, done = env.step(actions) 
        
        episode_contributions = [0.0] * num_agents
        episode_rewards = rewards 
        
        for agent_idx in range(num_agents):
            agents[agent_idx].update(states[agent_idx], actions[agent_idx], rewards[agent_idx], next_states[agent_idx])
            
            # Contribution for this single interaction (episode)
            contribution = actions[agent_idx] * env.endowments[agent_idx]
            episode_contributions[agent_idx] = contribution 
        
        states = next_states # Current states become next_states for the next episode's start
        
        # Calculate and save metrics every 100 episodes
        if (episode + 1) % 100 == 0:
            episode_metrics = calculate_metrics(episode_contributions, episode_rewards, env.r, env.endowments)
            
            for key, value in episode_metrics.items():
                if key in metrics_history:
                    metrics_history[key].append(value)
            
            print(f"Episode {episode + 1}/{num_episodes} - Avg Contribution: {episode_metrics.get('avg_contribution', 0.0):.4f}, Social Welfare: {episode_metrics.get('social_welfare', 0.0):.4f}")
            
            # Save metrics to CSV every 100 episodes
            metrics_df_data = {'episode': range(100, episode + 2, 100)}  # Episodes where metrics are calculated
            for key, value_list in metrics_history.items():
                metrics_df_data[key] = value_list

            metrics_df = pd.DataFrame(metrics_df_data)
            metrics_file = os.path.join(output_dir, f"{algorithm}_metrics.csv")
            metrics_df.to_csv(metrics_file, index=False)
            print(f"Metrics saved to {metrics_file}")
    
    # Final save to include all episodes
    metrics_df_data = {'episode': range(100, num_episodes + 1, 100)}
    for key, value_list in metrics_history.items():
        metrics_df_data[key] = value_list

    metrics_df = pd.DataFrame(metrics_df_data)
    metrics_file = os.path.join(output_dir, f"{algorithm}_metrics.csv")
    metrics_df.to_csv(metrics_file, index=False)
    print(f"Metrics saved to {metrics_file}")
    
    # Save Q-values for each agent (final Q-tables)
    for i, agent in enumerate(agents):
        q_table_file = os.path.join(output_dir, f"{algorithm}_agent_{i}_final_q_table.csv")
        q_values_to_save = {} # Default to an empty dict

        if isinstance(agent, QAgent):
            # Assuming QAgent has a method get_q_values() that returns its Q-table as a dict
            q_values_to_save = agent.get_q_values()
        elif isinstance(agent, DoubleQAgent):
            # Attempt to get q_table_A. If it's a tuple, try to convert to dict.
            potential_q_table_A = agent.q_table_A # Accessing the attribute as per previous logic
            
            if isinstance(potential_q_table_A, dict):
                q_values_to_save = potential_q_table_A
            elif isinstance(potential_q_table_A, tuple):
                try:
                    # If agent.q_table_A is a tuple of (key, value) pairs, convert to dict
                    q_values_to_save = dict(potential_q_table_A)
                    print(f"Info: Converted q_table_A (tuple) to dict for DoubleQAgent {i}.")
                except (TypeError, ValueError) as e:
                    print(f"Warning: DoubleQAgent {i}'s q_table_A is a tuple but could not be converted to a dict: {e}. Q-table for Q_A will be empty.")
                    q_values_to_save = {} # Fallback to empty dict
            else:
                print(f"Warning: DoubleQAgent {i}'s q_table_A is of unexpected type {type(potential_q_table_A)}. Q-table for Q_A will be empty.")
                q_values_to_save = {} # Fallback to empty dict
        
        if not q_values_to_save: # Check if q_values_to_save is empty after attempts
            print(f"Notice: Q-table for agent {i} ({agent.__class__.__name__}) is empty or was not retrievable as a dict. Skipping save for this Q-table.")
            continue # Skip to the next agent if no Q-table data

        try:
            # Convert Q-table keys (tuples like (state_tuple, action_float)) to strings for CSV compatibility
            q_table_to_save_str_keys = {str(k): v for k, v in q_values_to_save.items()}
            if not q_table_to_save_str_keys: # If conversion results in empty dict (e.g. q_values_to_save was empty)
                 print(f"Notice: Q-table for agent {i} ({agent.__class__.__name__}) resulted in an empty string-keyed dictionary. Skipping save.")
                 continue

            q_df = pd.DataFrame(list(q_table_to_save_str_keys.items()), columns=['state_action', 'q_value'])
            q_df.to_csv(q_table_file, index=False)
            print(f"Final Q-table for agent {i} saved to {q_table_file}")
        except AttributeError:
             # This might happen if q_values_to_save was not successfully made a dict and was e.g. None or still a non-dict tuple
             print(f"Error: Could not process Q-table for agent {i} due to AttributeError (likely not a dict). Q-table data: {q_values_to_save}")
        except Exception as e:
            print(f"Error saving Q-table for agent {i}: {e}")
