import os
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Union, Any
import random
from pathlib import Path
import math # Added for factorial

# Import environment and agents
from pgg_environment import PublicGoodsGame
from q_agent import QAgent
from double_q_agent import DoubleQAgent

def calculate_shapley_value(contributions: List[float], r: float) -> List[float]:
    """
    Calculate Shapley values for each player in the public goods game.
    
    Args:
        contributions: List of contributions from each player
        r: Multiplier for public goods
        
    Returns:
        List of Shapley values for each player
    """
    n = len(contributions)
    shapley_values = [0.0] * n
    
    memo_v = {} # Memoization for v(coalition_tuple)

    def v(coalition_indices: List[int]) -> float:
        """Calculate value of a coalition"""
        coalition_tuple = tuple(sorted(coalition_indices)) # Use a hashable type for memoization
        if not coalition_tuple:
            return 0.0
        if coalition_tuple in memo_v:
            return memo_v[coalition_tuple]
        
        coalition_sum = sum(contributions[i] for i in coalition_tuple)
        # Characteristic function: value generated by the coalition, shared among its members
        if not coalition_tuple: # Should be caught by the first check, but defensive
            result = 0.0
        else:
            result = (r * coalition_sum) / len(coalition_tuple) 
        memo_v[coalition_tuple] = result
        return result
    
    players = list(range(n))
    for i in range(n): # For each player i
        other_players = [p for p in players if p != i]
        for k in range(1 << (n - 1)): # Iterate through all subsets of other_players
            coalition_S = [] # Coalition S (does not include player i)
            for j in range(n - 1):
                if (k >> j) & 1:
                    coalition_S.append(other_players[j])
            
            s = len(coalition_S)
            # Corrected Shapley weight: s! * (n - 1 - s)! / n!
            if n == 0: 
                weight = 0.0
            else:
                try:
                    weight = (math.factorial(s) * math.factorial(n - 1 - s)) / math.factorial(n)
                except ValueError: 
                    weight = 0.0 

            # Value of coalition S with player i minus value of coalition S without player i
            val_S_union_i = v(coalition_S + [i])
            val_S = v(coalition_S)
            marginal_contribution = val_S_union_i - val_S
            shapley_values[i] += weight * marginal_contribution
            
    return shapley_values

def calculate_metrics(episode_contributions: List[float], episode_rewards: List[float], r: float, endowments: List[float]) -> Dict[str, float]:
    """Calculate metrics for the current episode."""
    n_agents = len(episode_contributions)
    if n_agents == 0: 
        return {
            'avg_contribution': 0.0,
            'total_contribution': 0.0,
            'social_welfare': 0.0,
            'fairness': 1.0, 
            'action_diversity': 0.0
        }

    # Basic metrics
    avg_contribution = np.mean(episode_contributions) if episode_contributions else 0.0
    total_contribution = np.sum(episode_contributions)
    social_welfare = np.sum(episode_rewards) # episode_rewards are now per-episode, not per-step sums
    
    # Calculate Shapley values
    shapley_values = calculate_shapley_value(episode_contributions, r)
    
    norm_contributions = []
    if len(endowments) == n_agents:
        norm_contributions = [(c / endowments[i] * 100) if endowments[i] > 0 else 0 for i, c in enumerate(episode_contributions)]
    else: 
        norm_contributions = [0.0] * n_agents

    max_sv = max(shapley_values) if shapley_values else 0.0
    min_sv = min(shapley_values) if shapley_values else 0.0
    
    # Adjusted fairness calculation for clarity and robustness
    if not shapley_values: # Handle empty Shapley values
        fairness_metric = 1.0 # Or 0.0, depending on desired behavior for no agents/no SVs
    elif max_sv == min_sv: # All Shapley values are equal (perfect fairness)
        fairness_metric = 1.0
    elif max_sv == 0 and min_sv == 0: # All SVs are zero
        fairness_metric = 1.0
    elif abs(max_sv) < 1e-9: # Max SV is effectively zero, avoid division by zero if min_sv is also small
        fairness_metric = 0.0 # Or handle as undefined/error if min_sv is significantly different
    else:
        fairness_metric = 1.0 - (max_sv - min_sv) / (abs(max_sv) + 1e-9) # Normalize by absolute max_sv

    metrics = {
        'avg_contribution': avg_contribution,
        'total_contribution': total_contribution,
        'social_welfare': social_welfare,
        'fairness': fairness_metric,
        'action_diversity': len(set(norm_contributions)) / n_agents if n_agents > 0 else 0.0,
        **{f'agent_{i}_contrib': contrib for i, contrib in enumerate(episode_contributions)},
        **{f'agent_{i}_norm_contrib': norm_contrib for i, norm_contrib in enumerate(norm_contributions)},
        **{f'agent_{i}_shapley': sv for i, sv in enumerate(shapley_values)}
    }
    
    return metrics

def run_simulation(
    algorithm: str,
    num_episodes: int,
    # steps_per_episode: int, # Removed
    env: PublicGoodsGame,
    agents: List[Union[QAgent, DoubleQAgent]],
    output_dir: str
) -> Dict[str, List[float]]:
    """
    Run MARL simulation, return and save metrics. Each episode is a single interaction.
    """
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    num_agents = env.num_agents

    metrics_history = {
        "avg_contribution": [],
        "total_contribution": [],
        "social_welfare": [],
        "fairness": [],
        "action_diversity": []
    }
    for i in range(num_agents):
        metrics_history[f"agent_{i}_contrib"] = []
        metrics_history[f"agent_{i}_norm_contrib"] = []
        metrics_history[f"agent_{i}_shapley"] = []
    
    states = env.reset() # Initial reset
    
    for episode in range(num_episodes):
        # If not the first episode, reset for the new episode
        if episode > 0:
            states = env.reset() 
        
        actions = []
        for agent_idx in range(num_agents):
            action = agents[agent_idx].choose_action(states[agent_idx])
            actions.append(action)
        
        next_states, rewards, done = env.step(actions) 
        
        episode_contributions = [0.0] * num_agents
        # Rewards from env.step are the rewards for this single interaction (episode)
        episode_rewards = rewards 
        
        for agent_idx in range(num_agents):
            agents[agent_idx].update(states[agent_idx], actions[agent_idx], rewards[agent_idx], next_states[agent_idx])
            
            # Contribution for this single interaction (episode)
            contribution = actions[agent_idx] * env.endowments[agent_idx]
            episode_contributions[agent_idx] = contribution 
        
        states = next_states # Current states become next_states for the next episode's start
        
        # Calculate metrics for the episode
        episode_metrics = calculate_metrics(episode_contributions, episode_rewards, env.r, env.endowments)
        
        for key, value in episode_metrics.items():
            if key in metrics_history:
                 metrics_history[key].append(value)
        
        if episode % 100 == 0:
            print(f"Episode {episode}/{num_episodes} - Avg Contribution: {episode_metrics.get('avg_contribution', 0.0):.4f}, Social Welfare: {episode_metrics.get('social_welfare', 0.0):.4f}")
    
    metrics_df_data = {'episode': range(num_episodes)}
    for key, value_list in metrics_history.items():
        if len(value_list) == num_episodes:
            metrics_df_data[key] = value_list
        else:
            metrics_df_data[key] = value_list + [np.nan] * (num_episodes - len(value_list))

    metrics_df = pd.DataFrame(metrics_df_data)
    metrics_file = os.path.join(output_dir, f"{algorithm}_metrics.csv")
    metrics_df.to_csv(metrics_file, index=False)
    print(f"Metrics saved to {metrics_file}")
    
    # Save Q-values for each agent (final Q-tables)
    for i, agent in enumerate(agents):
        q_table_file = os.path.join(output_dir, f"{algorithm}_agent_{i}_final_q_table.csv")
        q_values_to_save = {} # Default to an empty dict

        if isinstance(agent, QAgent):
            # Assuming QAgent has a method get_q_values() that returns its Q-table as a dict
            q_values_to_save = agent.get_q_values()
        elif isinstance(agent, DoubleQAgent):
            # Attempt to get q_table_A. If it's a tuple, try to convert to dict.
            potential_q_table_A = agent.q_table_A # Accessing the attribute as per previous logic
            
            if isinstance(potential_q_table_A, dict):
                q_values_to_save = potential_q_table_A
            elif isinstance(potential_q_table_A, tuple):
                try:
                    # If agent.q_table_A is a tuple of (key, value) pairs, convert to dict
                    q_values_to_save = dict(potential_q_table_A)
                    print(f"Info: Converted q_table_A (tuple) to dict for DoubleQAgent {i}.")
                except (TypeError, ValueError) as e:
                    print(f"Warning: DoubleQAgent {i}'s q_table_A is a tuple but could not be converted to a dict: {e}. Q-table for Q_A will be empty.")
                    q_values_to_save = {} # Fallback to empty dict
            else:
                print(f"Warning: DoubleQAgent {i}'s q_table_A is of unexpected type {type(potential_q_table_A)}. Q-table for Q_A will be empty.")
                q_values_to_save = {} # Fallback to empty dict
        
        if not q_values_to_save: # Check if q_values_to_save is empty after attempts
            print(f"Notice: Q-table for agent {i} ({agent.__class__.__name__}) is empty or was not retrievable as a dict. Skipping save for this Q-table.")
            continue # Skip to the next agent if no Q-table data

        try:
            # Convert Q-table keys (tuples like (state_tuple, action_float)) to strings for CSV compatibility
            q_table_to_save_str_keys = {str(k): v for k, v in q_values_to_save.items()}
            if not q_table_to_save_str_keys: # If conversion results in empty dict (e.g. q_values_to_save was empty)
                 print(f"Notice: Q-table for agent {i} ({agent.__class__.__name__}) resulted in an empty string-keyed dictionary. Skipping save.")
                 continue

            q_df = pd.DataFrame(list(q_table_to_save_str_keys.items()), columns=['state_action', 'q_value'])
            q_df.to_csv(q_table_file, index=False)
            print(f"Final Q-table for agent {i} saved to {q_table_file}")
        except AttributeError:
             # This might happen if q_values_to_save was not successfully made a dict and was e.g. None or still a non-dict tuple
             print(f"Error: Could not process Q-table for agent {i} due to AttributeError (likely not a dict). Q-table data: {q_values_to_save}")
        except Exception as e:
            print(f"Error saving Q-table for agent {i}: {e}")


    # Save detailed decision log if enabled
